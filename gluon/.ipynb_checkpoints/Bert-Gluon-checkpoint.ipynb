{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, autograd\n",
    "import gluonnlp as nlp\n",
    "from bert import *\n",
    "from gluonnlp.data import TSVDataset\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from glob import glob\n",
    "\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)\n",
    "ctx = mx.gpu(0) if mx.test_utils.list_gpus() else mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '/home/yaserkl/data/imdb/v1/'\n",
    "working_dir = \"/home/yaserkl/working_dir/classification/gluon/bert/imdb/\"\n",
    "if not os.path.exists(working_dir):\n",
    "    os.makedirs(working_dir)\n",
    "filename = '{}/net.params'.format(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network params\n",
    "# maximum sequence length\n",
    "max_len = 150\n",
    "n_classes = 2\n",
    "all_labels = [str(_) for _ in range(n_classes)]\n",
    "batch_size = 32\n",
    "lr = 5e-6\n",
    "grad_clip = 1\n",
    "log_interval = 4\n",
    "num_epochs = 10\n",
    "max_patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(y_true, y_pred):\n",
    "    print(f1_score(y_true, y_pred, average=\"macro\"))\n",
    "    print(precision_score(y_true, y_pred, average=\"macro\"))\n",
    "    print(recall_score(y_true, y_pred, average=\"macro\"))\n",
    "    fig, ax = plt.subplots(figsize=(19, 10))\n",
    "    x = sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, ax=ax)\n",
    "    x.invert_yaxis()\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    return fig\n",
    "\n",
    "class Dataset(TSVDataset):\n",
    "    \"\"\"Train dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    segment : str or list of str, default 'train'\n",
    "        Dataset segment. Options are 'train', 'val', 'test' or their combinations.\n",
    "    root : str, default 'dir containing train/dev/test datasets'\n",
    "    \"\"\"\n",
    "    def __init__(self, segment='train', root=ROOT_DIR, n_classes=2):\n",
    "        self._supported_segments = ['train', 'dev', 'test']\n",
    "        assert segment in self._supported_segments, 'Unsupported segment: %s'%segment\n",
    "        path = os.path.join(root, '%s.tsv'%segment)\n",
    "        A_IDX, LABEL_IDX = 0, 1\n",
    "        fields = [A_IDX, LABEL_IDX]\n",
    "        self.n_classes=n_classes\n",
    "        super(Dataset, self).__init__(path, field_indices=fields)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_labels():\n",
    "        \"\"\"Get classification label ids of the dataset.\"\"\"\n",
    "        return [str(_) for _ in range(self.n_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTModel(\n",
      "  (encoder): BERTEncoder(\n",
      "    (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "    (layer_norm): BERTLayerNorm(in_channels=768, epsilon=1e-12)\n",
      "    (transformer_cells): HybridSequential(\n",
      "      (0): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(in_channels=768, epsilon=1e-12)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(in_channels=768, epsilon=1e-12)\n",
      "      )\n",
      "      (1): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(in_channels=768, epsilon=1e-12)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(in_channels=768, epsilon=1e-12)\n",
      "      )\n",
      "      (2): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(in_channels=768, epsilon=1e-12)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(in_channels=768, epsilon=1e-12)\n",
      "      )\n",
      "      (3): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(in_channels=768, epsilon=1e-12)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(in_channels=768, epsilon=1e-12)\n",
      "      )\n",
      "      (4): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(in_channels=768, epsilon=1e-12)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(in_channels=768, epsilon=1e-12)\n",
      "      )\n",
      "      (5): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(in_channels=768, epsilon=1e-12)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(in_channels=768, epsilon=1e-12)\n",
      "      )\n",
      "      (6): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(in_channels=768, epsilon=1e-12)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(in_channels=768, epsilon=1e-12)\n",
      "      )\n",
      "      (7): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(in_channels=768, epsilon=1e-12)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(in_channels=768, epsilon=1e-12)\n",
      "      )\n",
      "      (8): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(in_channels=768, epsilon=1e-12)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(in_channels=768, epsilon=1e-12)\n",
      "      )\n",
      "      (9): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(in_channels=768, epsilon=1e-12)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(in_channels=768, epsilon=1e-12)\n",
      "      )\n",
      "      (10): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(in_channels=768, epsilon=1e-12)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(in_channels=768, epsilon=1e-12)\n",
      "      )\n",
      "      (11): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(in_channels=768, epsilon=1e-12)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(in_channels=768, epsilon=1e-12)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_embed): HybridSequential(\n",
      "    (0): Embedding(30522 -> 768, float32)\n",
      "    (1): Dropout(p = 0.1, axes=())\n",
      "  )\n",
      "  (token_type_embed): HybridSequential(\n",
      "    (0): Embedding(2 -> 768, float32)\n",
      "    (1): Dropout(p = 0.1, axes=())\n",
      "  )\n",
      "  (pooler): Dense(768 -> 768, Activation(tanh))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bert_base, vocabulary = nlp.model.get_model('bert_12_768_12',\n",
    "                                             dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                             pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                                             use_decoder=False, use_classifier=False)\n",
    "print(bert_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = Dataset('train', n_classes=n_classes)\n",
    "data_dev = Dataset('dev', n_classes=n_classes)\n",
    "data_test = Dataset('test', n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utterly pretentious nonsense the material is dull , dull , dull , and most of the cast would n't even have made understudies in allen 's earlier films and to have to listen to the unfunny will ferrell do his woody allen imitation makes me loathe the second rate \\\\( though mysteriously popular \\\\) ferrell even more it appears that the morose 70 year old allen should have knocked off work when the clock rang in a new century br br i truly tried to get involved in the film , but it was just impossible my snyapses could n't fire that slowly so , rather than doze off and kill the afternoon sleeping in an upright position i got up , left my wife and daughter in the theater , and went out to the car where i had a really good book to re read \\\\( george bailey 's great tome of 30 years ago , germans \\\\) the day turned out pretty well after all , no thanks to woody\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "sample_id = 0\n",
    "# sentence a\n",
    "print(data_train[sample_id][0])\n",
    "# label\n",
    "print(data_train[sample_id][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the vocabulary from pre-trained model for tokenization\n",
    "tokenizer = tokenization.FullTokenizer(vocabulary, do_lower_case=True)\n",
    "transform = dataset.ClassificationTransform(tokenizer, all_labels, max_len, pair=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ids = \n",
      "[    2 12580  3653  6528 20771 14652  1996  3430  2003 10634  1010 10634\n",
      "  1010 10634  1010  1998  2087  1997  1996  3459  2052  1050  1005  1056\n",
      "  2130  2031  2081  2104  3367 21041  2229  1999  5297  1005  1055  3041\n",
      "  3152  1998  2000  2031  2000  4952  2000  1996  4895 11263 10695  2100\n",
      "  2097 10768 14069  2079  2010 13703  5297 20017  3084  2033  8840  8988\n",
      "  2063  1996  2117  3446  1032  1032  1006  2295 29239  2759  1032  1032\n",
      "  1007 10768 14069  2130  2062  2009  3544  2008  1996 22822  9232  3963\n",
      "  2095  2214  5297  2323  2031  6573  2125  2147  2043  1996  5119  8369\n",
      "  1999  1037  2047  2301  7987  7987  1045  5621  2699  2000  2131  2920\n",
      "  1999  1996  2143  1010  2021  2009  2001  2074  5263  2026  1055 17238\n",
      " 29251  2015  2071  1050  1005  1056  2543  2008  3254  2061  1010  2738\n",
      "  2084  2079  4371  2125  1998  3102  1996  5027  5777  1999  2019 10051\n",
      "  2597  1045  2288  2039  1010     3]\n"
     ]
    }
   ],
   "source": [
    "data_train = data_train.transform(transform)\n",
    "data_dev = data_dev.transform(transform)\n",
    "data_test = data_test.transform(transform)\n",
    "print('token ids = \\n%s'%data_train[sample_id][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = mx.gluon.data.DataLoader(data_train, batch_size=batch_size, shuffle=True, last_batch='rollover')\n",
    "dev_dataloader = mx.gluon.data.DataLoader(data_dev, batch_size=batch_size, shuffle=False, last_batch='rollover')\n",
    "test_dataloader = mx.gluon.data.DataLoader(data_dev, batch_size=batch_size, shuffle=False, last_batch='rollover')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bert.BERTClassifier(bert_base, num_classes=n_classes, dropout=0.1)\n",
    "# only need to initialize the classifier layer.\n",
    "model.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "model.hybridize(static_alloc=True)\n",
    "\n",
    "# softmax cross entropy loss for classification\n",
    "loss_function = gluon.loss.SoftmaxCELoss()\n",
    "loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "metric = mx.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4c1e4628e509>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             print('[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f}'\n\u001b[0;32m---> 40\u001b[0;31m                          .format(epoch_id, batch_id + 1, len(train_dataloader),\n\u001b[0m\u001b[1;32m     41\u001b[0m                                  \u001b[0mstep_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                                  trainer.learning_rate, metric.get()[1]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch_id' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': lr, 'epsilon': 1e-9})\n",
    "\n",
    "# collect all differentiable parameters\n",
    "# grad_req == 'null' indicates no gradients are calculated (e.g. constant parameters)\n",
    "# the gradients for these params are clipped later\n",
    "params = [p for p in model.collect_params().values() if p.grad_req != 'null']\n",
    "\n",
    "train_step = 0\n",
    "epoch_id = 0\n",
    "best_loss = None\n",
    "patience = 0\n",
    "#for epoch_id in range(num_epochs):\n",
    "while True:\n",
    "    metric.reset()\n",
    "    step_loss = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(train_dataloader):\n",
    "        # load data to GPU\n",
    "        token_ids = token_ids.as_in_context(ctx)\n",
    "        valid_length = valid_length.as_in_context(ctx)\n",
    "        segment_ids = segment_ids.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "\n",
    "        with autograd.record():\n",
    "            # forward computation\n",
    "            out = model(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "            ls = loss_function(out, label).mean()\n",
    "\n",
    "        # backward computation\n",
    "        ls.backward()\n",
    "\n",
    "        # gradient clipping\n",
    "        grads = [p.grad(c) for p in params for c in [ctx]]\n",
    "        gluon.utils.clip_global_norm(grads, grad_clip)\n",
    "\n",
    "        # parameter update\n",
    "        trainer.step(1)\n",
    "        step_loss += ls.asscalar()\n",
    "        metric.update([label], [out])\n",
    "        if (batch_id + 1) % (log_interval) == 0:\n",
    "            print('[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f}'\n",
    "                         .format(epoch_id, batch_id + 1, len(train_dataloader),\n",
    "                                 step_loss / log_interval,\n",
    "                                 trainer.learning_rate, metric.get()[1]))\n",
    "            step_loss = 0\n",
    "        train_step +=1\n",
    "    epoch_id+=1\n",
    "    ########################\n",
    "    #### RUN EVALUATION ####\n",
    "    ########################\n",
    "    dev_loss = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(dev_dataloader):\n",
    "        # load data to GPU\n",
    "        token_ids = token_ids.as_in_context(ctx)\n",
    "        valid_length = valid_length.as_in_context(ctx)\n",
    "        segment_ids = segment_ids.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        out = model(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "        ls = loss_function(out, label).mean()\n",
    "        dev_loss.append(ls.asscalar())\n",
    "        probs = out.softmax()\n",
    "        pred = nd.argmax(probs, axis=1).asnumpy()\n",
    "        y_true.extend(list(np.reshape(label.asnumpy(), (-1))))\n",
    "        y_pred.extend(pred)\n",
    "    dev_loss = np.mean(dev_loss)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print('EVALUATION ON DEV DATASET:')\n",
    "    print('dev mean loss: {:.4f}, f1-score: {:.4f}, accuracy: {:0.4f}'.format(dev_loss, f1, acc))\n",
    "    if best_loss is None or dev_loss < best_loss:\n",
    "        model.save_parameters('{}_{}_best'.format(filename, train_step))\n",
    "        best_loss = dev_loss\n",
    "        patience=0\n",
    "    else:\n",
    "        new_lr = trainer.learning_rate/2\n",
    "        trainer.set_learning_rate(new_lr)\n",
    "        print('patience #{}: reducing the lr to {}'.format(patience, new_lr))\n",
    "        patience+=1\n",
    "    if patience == max_patience:\n",
    "        model.save_parameters('{}_{}'.format(filename, train_step))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best pre-trained model for evaluation\n",
    "best_ckpt = glob('{}*best'.format(filename))[0]\n",
    "model = bert.BERTClassifier(bert_base, num_classes=n_classes, dropout=0.1)\n",
    "model.load_parameters(best_ckpt, ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "    token_ids = token_ids.as_in_context(ctx)\n",
    "    valid_length = valid_length.as_in_context(ctx)\n",
    "    segment_ids = segment_ids.as_in_context(ctx)\n",
    "    label = label.as_in_context(ctx)\n",
    "    out = model(token_ids, segment_ids, valid_length.astype('float32')).softmax()\n",
    "    pred = nd.argmax(out, axis=1).asnumpy()\n",
    "    y_true.extend(list(np.reshape(label.asnumpy(), (-1))))\n",
    "    y_pred.extend(pred)\n",
    "assert len(y_true)==len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = print_results(np.reshape(y_true, (-1)), y_pred)\n",
    "fig.savefig('{}/cm.png'.format(working_dir))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
