{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, autograd\n",
    "import gluonnlp as nlp\n",
    "from bert import *\n",
    "from gluonnlp.data import TSVDataset\n",
    "from glob import glob\n",
    "from os.path import expanduser\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "# seeding all randomizers\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(100)\n",
    "\n",
    "# use GPU when available otherwise use CPU\n",
    "ctx = mx.gpu(0) if mx.test_utils.list_gpus() else mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = '20ng'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclasses = {\"20ng\":20, \"imdb\":2, \"r8\":8, \"r52\":52, \"ohsumed_all\": 23, \"ohsumed_first\": 23}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = expanduser(\"~\")\n",
    "DATADIR = '{}/working_dir/classification/files/{}/'.format(HOME, dataset_name)\n",
    "working_dir = \"{}/working_dir/classification/models/bert/{}/\".format(HOME, dataset_name)\n",
    "if not os.path.exists(working_dir):\n",
    "    os.makedirs(working_dir)\n",
    "filename = '{}/net.params'.format(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network params\n",
    "# maximum sequence length\n",
    "max_len = 150\n",
    "# number of classes\n",
    "n_classes = nclasses[dataset_name]\n",
    "all_labels = [str(_) for _ in range(n_classes)]\n",
    "# batch size\n",
    "batch_size = 32\n",
    "# initial learning rate\n",
    "lr = 5e-6\n",
    "# gradient clipping value\n",
    "grad_clip = 1\n",
    "# log to screen every 50 batch\n",
    "log_interval = 50\n",
    "# train until we fail to beat the current best validation loss for 5 consecutive epochs\n",
    "max_patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(y_true, y_pred):\n",
    "    print(\"Accuracy: {:0.3f}\".format(accuracy_score(y_true, y_pred)))\n",
    "    print(\"F1-Score: {:0.3f}\".format(f1_score(y_true, y_pred, average=\"macro\")))\n",
    "    print(\"Precision: {:0.3f}\".format(precision_score(y_true, y_pred, average=\"macro\")))\n",
    "    print(\"Recall: {:0.3f}\".format(recall_score(y_true, y_pred, average=\"macro\")))\n",
    "    fig, ax = plt.subplots(figsize=(18, 10))\n",
    "    x = sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, ax=ax)\n",
    "    x.invert_yaxis()\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    return fig\n",
    "\n",
    "class Dataset(TSVDataset):\n",
    "    \"\"\"Train dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    segment : str or list of str, default 'train'\n",
    "        Dataset segment. Options are 'train', 'val', 'test' or their combinations.\n",
    "    root : str, default 'dir containing train/dev/test datasets'\n",
    "    \"\"\"\n",
    "    def __init__(self, segment='train', root='.', n_classes=2):\n",
    "        self._supported_segments = ['train', 'dev', 'test']\n",
    "        assert segment in self._supported_segments, 'Unsupported segment: %s'%segment\n",
    "        path = os.path.join(root, '%s.tsv'%segment)\n",
    "        A_IDX, LABEL_IDX = 0, 1\n",
    "        fields = [A_IDX, LABEL_IDX]\n",
    "        self.n_classes=n_classes\n",
    "        super(Dataset, self).__init__(path, field_indices=fields)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_labels():\n",
    "        \"\"\"Get classification label ids of the dataset.\"\"\"\n",
    "        return [str(_) for _ in range(self.n_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = Dataset(root=DATADIR, segment='train', n_classes=n_classes)\n",
    "data_dev = Dataset(root=DATADIR, segment='dev', n_classes=n_classes)\n",
    "data_test = Dataset(root=DATADIR, segment='test', n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<TEXT>>>>\n",
      "from steph pegasus cs uiuc edu \\\\( dale stephenson \\\\) subject re rockies \\\\( not rookies \\\\) article i d pegasus steph 733996812 organization university of illinois , dept of comp sci , urbana , il lines 19 in c500u7 kr8 news cso uiuc edu dbl50872 uxa cso uiuc edu \\\\( daniel brian lake \\\\) writes you 'd think that an expansion team would be filled with young'ns , not guys like murphy , galaragga , b smith it depends if you can get your old veterans cheap , and if they can perform at a higher level than your young talent can now , why not the talent develop in the minors while giving the fans some familiar names to cheer if the veterans are gone in a year or two that should be just about right maybe someone should tell those renegade front office people in denver \\\\) open question which was more important to the expansion clubs , the expansion draft or the regular draft \\\\( they 've had one of each , i think \\\\) dale j stephenson \\\\( steph cs uiuc edu \\\\) baseball fanatic it is considered good to look wise , especially when not overburdened with information j golden kimball\n",
      "<<<<LABEL>>>>\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "sample_id = np.random.randint(0, len(data_train))\n",
    "print('<<<<TEXT>>>>')\n",
    "print(data_train[sample_id][0])\n",
    "print(\"<<<<LABEL>>>>\")\n",
    "print(data_train[sample_id][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_base, vocabulary = nlp.model.get_model('bert_12_768_12',\n",
    "                                             dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                             pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                                             use_decoder=False, use_classifier=False)\n",
    "#print(bert_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the vocabulary from pre-trained model for tokenization\n",
    "tokenizer = tokenization.FullTokenizer(vocabulary, do_lower_case=True)\n",
    "transform = dataset.ClassificationTransform(tokenizer, all_labels, max_len, pair=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ids = \n",
      "[    2  2013  3357  2232 26606 20116 21318 14194  3968  2226  1032  1032\n",
      "  1006  8512 19789  1032  1032  1007  3395  2128 22366  1032  1032  1006\n",
      "  2025  8305  2015  1032  1032  1007  3720  1045  1040 26606  3357  2232\n",
      "  6421 23499  2683  2575  2620 12521  3029  2118  1997  4307  1010 29466\n",
      "  1997  4012  2361 16596  1010 27929  1010  6335  3210  2539  1999  1039\n",
      " 29345  2226  2581  1047  2099  2620  2739 20116  2080 21318 14194  3968\n",
      "  2226 16962  2140 12376  2620  2581  2475  1057 18684 20116  2080 21318\n",
      " 14194  3968  2226  1032  1032  1006  3817  4422  2697  1032  1032  1007\n",
      "  7009  2017  1005  1040  2228  2008  2019  4935  2136  2052  2022  3561\n",
      "  2007  2402  1005 24978  1010  2025  4364  2066  7104  1010 16122 29181\n",
      "  3654  1010  1038  3044  2009  9041  2065  2017  2064  2131  2115  2214\n",
      "  8244 10036  1010  1998  2065  2027  2064  4685  2012  1037  3020  2504\n",
      "  2084  2115  2402  5848  2064     3]\n"
     ]
    }
   ],
   "source": [
    "data_train = data_train.transform(transform)\n",
    "data_dev = data_dev.transform(transform)\n",
    "data_test = data_test.transform(transform)\n",
    "print('token ids = \\n%s'%data_train[sample_id][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = mx.gluon.data.DataLoader(data_train, batch_size=batch_size, shuffle=True, last_batch='rollover')\n",
    "dev_dataloader = mx.gluon.data.DataLoader(data_dev, batch_size=batch_size, shuffle=False, last_batch='rollover')\n",
    "test_dataloader = mx.gluon.data.DataLoader(data_dev, batch_size=batch_size, shuffle=False, last_batch='rollover')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bert.BERTClassifier(bert_base, num_classes=n_classes, dropout=0.1)\n",
    "# only need to initialize the classifier layer.\n",
    "model.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "model.hybridize(static_alloc=True)\n",
    "\n",
    "# softmax cross entropy loss for classification\n",
    "loss_function = gluon.loss.SoftmaxCELoss()\n",
    "loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "metric = mx.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 50/300] loss=3.0178, lr=0.0000050, acc=0.056\n",
      "[Epoch 0 Batch 100/300] loss=2.9149, lr=0.0000050, acc=0.088\n",
      "[Epoch 0 Batch 150/300] loss=2.6667, lr=0.0000050, acc=0.150\n",
      "[Epoch 0 Batch 200/300] loss=2.4379, lr=0.0000050, acc=0.205\n",
      "[Epoch 0 Batch 250/300] loss=2.2472, lr=0.0000050, acc=0.256\n",
      "[Epoch 0 Batch 300/300] loss=2.0707, lr=0.0000050, acc=0.302\n",
      "EVALUATION ON DEV DATASET:\n",
      "dev mean loss: 1.9840, f1-score: 0.4830, accuracy: 0.5336\n",
      "[Epoch 1 Batch 50/300] loss=1.9188, lr=0.0000050, acc=0.578\n",
      "[Epoch 1 Batch 100/300] loss=1.7727, lr=0.0000050, acc=0.590\n",
      "[Epoch 1 Batch 150/300] loss=1.6514, lr=0.0000050, acc=0.602\n",
      "[Epoch 1 Batch 200/300] loss=1.5138, lr=0.0000050, acc=0.620\n",
      "[Epoch 1 Batch 250/300] loss=1.4171, lr=0.0000050, acc=0.632\n",
      "[Epoch 1 Batch 300/300] loss=1.3687, lr=0.0000050, acc=0.640\n",
      "EVALUATION ON DEV DATASET:\n",
      "dev mean loss: 1.2783, f1-score: 0.6638, accuracy: 0.6916\n",
      "[Epoch 2 Batch 50/300] loss=1.2419, lr=0.0000050, acc=0.721\n",
      "[Epoch 2 Batch 100/300] loss=1.1763, lr=0.0000050, acc=0.726\n",
      "[Epoch 2 Batch 150/300] loss=1.0864, lr=0.0000050, acc=0.738\n",
      "[Epoch 2 Batch 200/300] loss=1.0477, lr=0.0000050, acc=0.745\n",
      "[Epoch 2 Batch 250/300] loss=0.9847, lr=0.0000050, acc=0.752\n",
      "[Epoch 2 Batch 300/300] loss=0.9420, lr=0.0000050, acc=0.759\n",
      "EVALUATION ON DEV DATASET:\n",
      "dev mean loss: 0.9206, f1-score: 0.7350, accuracy: 0.7553\n",
      "[Epoch 3 Batch 50/300] loss=0.8322, lr=0.0000050, acc=0.815\n",
      "[Epoch 3 Batch 100/300] loss=0.8242, lr=0.0000050, acc=0.815\n",
      "[Epoch 3 Batch 150/300] loss=0.8055, lr=0.0000050, acc=0.812\n",
      "[Epoch 3 Batch 200/300] loss=0.7861, lr=0.0000050, acc=0.812\n",
      "[Epoch 3 Batch 250/300] loss=0.7456, lr=0.0000050, acc=0.814\n",
      "[Epoch 3 Batch 300/300] loss=0.7273, lr=0.0000050, acc=0.814\n",
      "EVALUATION ON DEV DATASET:\n",
      "dev mean loss: 0.7110, f1-score: 0.7824, accuracy: 0.7995\n",
      "[Epoch 4 Batch 50/300] loss=0.6494, lr=0.0000050, acc=0.855\n",
      "[Epoch 4 Batch 100/300] loss=0.6275, lr=0.0000050, acc=0.854\n",
      "[Epoch 4 Batch 150/300] loss=0.6038, lr=0.0000050, acc=0.855\n",
      "[Epoch 4 Batch 200/300] loss=0.5992, lr=0.0000050, acc=0.856\n",
      "[Epoch 4 Batch 250/300] loss=0.5700, lr=0.0000050, acc=0.857\n",
      "[Epoch 4 Batch 300/300] loss=0.5430, lr=0.0000050, acc=0.858\n",
      "EVALUATION ON DEV DATASET:\n",
      "dev mean loss: 0.5922, f1-score: 0.8313, accuracy: 0.8384\n",
      "[Epoch 5 Batch 50/300] loss=0.4768, lr=0.0000050, acc=0.891\n",
      "[Epoch 5 Batch 100/300] loss=0.4953, lr=0.0000050, acc=0.887\n",
      "[Epoch 5 Batch 150/300] loss=0.4965, lr=0.0000050, acc=0.887\n",
      "[Epoch 5 Batch 200/300] loss=0.4511, lr=0.0000050, acc=0.890\n",
      "[Epoch 5 Batch 250/300] loss=0.4494, lr=0.0000050, acc=0.891\n",
      "[Epoch 5 Batch 300/300] loss=0.4395, lr=0.0000050, acc=0.892\n",
      "EVALUATION ON DEV DATASET:\n",
      "dev mean loss: 0.4858, f1-score: 0.8710, accuracy: 0.8721\n",
      "[Epoch 6 Batch 50/300] loss=0.3754, lr=0.0000050, acc=0.914\n",
      "[Epoch 6 Batch 100/300] loss=0.3800, lr=0.0000050, acc=0.915\n",
      "[Epoch 6 Batch 150/300] loss=0.3568, lr=0.0000050, acc=0.916\n",
      "[Epoch 6 Batch 200/300] loss=0.3632, lr=0.0000050, acc=0.916\n",
      "[Epoch 6 Batch 250/300] loss=0.3649, lr=0.0000050, acc=0.915\n",
      "[Epoch 6 Batch 300/300] loss=0.3649, lr=0.0000050, acc=0.915\n",
      "EVALUATION ON DEV DATASET:\n",
      "dev mean loss: 0.4354, f1-score: 0.8748, accuracy: 0.8791\n",
      "[Epoch 7 Batch 50/300] loss=0.2813, lr=0.0000050, acc=0.939\n",
      "[Epoch 7 Batch 100/300] loss=0.3068, lr=0.0000050, acc=0.932\n",
      "[Epoch 7 Batch 150/300] loss=0.3036, lr=0.0000050, acc=0.931\n",
      "[Epoch 7 Batch 200/300] loss=0.2837, lr=0.0000050, acc=0.933\n",
      "[Epoch 7 Batch 250/300] loss=0.2999, lr=0.0000050, acc=0.933\n"
     ]
    }
   ],
   "source": [
    "trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': lr, 'epsilon': 1e-9})\n",
    "\n",
    "# collect all differentiable parameters\n",
    "# grad_req == 'null' indicates no gradients are calculated (e.g. constant parameters)\n",
    "# the gradients for these params are clipped later\n",
    "params = [p for p in model.collect_params().values() if p.grad_req != 'null']\n",
    "\n",
    "train_step = 0\n",
    "epoch_id = 0\n",
    "best_loss = None\n",
    "patience = 0\n",
    "while True:\n",
    "    metric.reset()\n",
    "    step_loss = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(train_dataloader):\n",
    "        # load data to GPU\n",
    "        token_ids = token_ids.as_in_context(ctx)\n",
    "        valid_length = valid_length.as_in_context(ctx)\n",
    "        segment_ids = segment_ids.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "\n",
    "        with autograd.record():\n",
    "            # forward computation\n",
    "            out = model(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "            ls = loss_function(out, label).mean()\n",
    "\n",
    "        # backward computation\n",
    "        ls.backward()\n",
    "\n",
    "        # gradient clipping\n",
    "        grads = [p.grad(c) for p in params for c in [ctx]]\n",
    "        gluon.utils.clip_global_norm(grads, grad_clip)\n",
    "\n",
    "        # parameter update\n",
    "        trainer.step(1)\n",
    "        step_loss += ls.asscalar()\n",
    "        metric.update([label], [out])\n",
    "        if (batch_id + 1) % (log_interval) == 0:\n",
    "            print('[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f}'\n",
    "                         .format(epoch_id, batch_id + 1, len(train_dataloader),\n",
    "                                 step_loss / log_interval,\n",
    "                                 trainer.learning_rate, metric.get()[1]))\n",
    "            step_loss = 0\n",
    "        train_step +=1\n",
    "    epoch_id+=1\n",
    "    ########################\n",
    "    #### RUN EVALUATION ####\n",
    "    ########################\n",
    "    dev_loss = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(dev_dataloader):\n",
    "        # load data to GPU\n",
    "        token_ids = token_ids.as_in_context(ctx)\n",
    "        valid_length = valid_length.as_in_context(ctx)\n",
    "        segment_ids = segment_ids.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        # get logits and loss value\n",
    "        out = model(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "        ls = loss_function(out, label).mean()\n",
    "        dev_loss.append(ls.asscalar())\n",
    "        probs = out.softmax()\n",
    "        pred = nd.argmax(probs, axis=1).asnumpy()\n",
    "        y_true.extend(list(np.reshape(label.asnumpy(), (-1))))\n",
    "        y_pred.extend(pred)\n",
    "    dev_loss = np.mean(dev_loss)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print('EVALUATION ON DEV DATASET:')\n",
    "    print('dev mean loss: {:.4f}, f1-score: {:.4f}, accuracy: {:0.4f}'.format(dev_loss, f1, acc))\n",
    "    if best_loss is None or dev_loss < best_loss:\n",
    "        model.save_parameters('{}_best'.format(filename, train_step))\n",
    "        best_loss = dev_loss\n",
    "        patience=0\n",
    "    else:\n",
    "        new_lr = trainer.learning_rate/2\n",
    "        trainer.set_learning_rate(new_lr)\n",
    "        print('patience #{}: reducing the lr to {}'.format(patience, new_lr))\n",
    "        patience+=1\n",
    "    if patience == max_patience:\n",
    "        model.save_parameters('{}_{}'.format(filename, train_step))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best pre-trained model for evaluation\n",
    "best_ckpt = glob('{}*best'.format(filename))[0]\n",
    "model = bert.BERTClassifier(bert_base, num_classes=n_classes, dropout=0.1)\n",
    "model.load_parameters(best_ckpt, ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "    token_ids = token_ids.as_in_context(ctx)\n",
    "    valid_length = valid_length.as_in_context(ctx)\n",
    "    segment_ids = segment_ids.as_in_context(ctx)\n",
    "    label = label.as_in_context(ctx)\n",
    "    out = model(token_ids, segment_ids, valid_length.astype('float32')).softmax()\n",
    "    pred = nd.argmax(out, axis=1).asnumpy()\n",
    "    y_true.extend(list(np.reshape(label.asnumpy(), (-1))))\n",
    "    y_pred.extend(pred)\n",
    "assert len(y_true)==len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = print_results(np.reshape(y_true, (-1)), y_pred)\n",
    "fig.savefig('{}/cm.png'.format(working_dir))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
