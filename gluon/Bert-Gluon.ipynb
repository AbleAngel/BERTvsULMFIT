{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, autograd\n",
    "import gluonnlp as nlp\n",
    "from bert import *\n",
    "from gluonnlp.data import TSVDataset\n",
    "from glob import glob\n",
    "from os.path import expanduser\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "# seeding all randomizers\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(100)\n",
    "\n",
    "# use GPU when available otherwise use CPU\n",
    "ctx = mx.gpu(0) if mx.test_utils.list_gpus() else mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'imdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclasses = {\"20ng\":20, \"imdb\":2, \"r8\":8, \"r52\":52, \"ohsumed_all\": 23, \"ohsumed_first\": 23}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = expanduser(\"~\")\n",
    "DATADIR = '{}/working_dir/classification/files/{}/'.format(HOME, dataset_name)\n",
    "working_dir = \"{}/working_dir/classification/models/bert/{}/\".format(HOME, dataset_name)\n",
    "if not os.path.exists(working_dir):\n",
    "    os.makedirs(working_dir)\n",
    "filename = '{}/net.params'.format(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network params\n",
    "# maximum sequence length\n",
    "max_len = 150\n",
    "# number of classes\n",
    "n_classes = nclasses[dataset_name]\n",
    "all_labels = [str(_) for _ in range(n_classes)]\n",
    "# batch size\n",
    "batch_size = 32\n",
    "# initial learning rate\n",
    "lr = 5e-6\n",
    "# gradient clipping value\n",
    "grad_clip = 1\n",
    "# log to screen every 50 batch\n",
    "log_interval = 50\n",
    "# train until we fail to beat the current best validation loss for 5 consecutive epochs\n",
    "max_patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(y_true, y_pred):\n",
    "    print(\"Accuracy: {:2.2f}\".format(100*accuracy_score(y_true, y_pred)))\n",
    "    print(\"F1-Score: {:2.2f}\".format(100*f1_score(y_true, y_pred, average=\"macro\")))\n",
    "    print(\"Precision: {:2.2f}\".format(100*precision_score(y_true, y_pred, average=\"macro\")))\n",
    "    print(\"Recall: {:2.2f}\".format(100*recall_score(y_true, y_pred, average=\"macro\")))\n",
    "    fig, ax = plt.subplots(figsize=(18, 10))\n",
    "    x = sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, ax=ax)\n",
    "    x.invert_yaxis()\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    return fig\n",
    "\n",
    "class Dataset(TSVDataset):\n",
    "    \"\"\"Train dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    segment : str or list of str, default 'train'\n",
    "        Dataset segment. Options are 'train', 'val', 'test' or their combinations.\n",
    "    root : str, default 'dir containing train/dev/test datasets'\n",
    "    \"\"\"\n",
    "    def __init__(self, segment='train', root='.', n_classes=2):\n",
    "        self._supported_segments = ['train', 'dev', 'test']\n",
    "        assert segment in self._supported_segments, 'Unsupported segment: %s'%segment\n",
    "        path = os.path.join(root, '%s.tsv'%segment)\n",
    "        A_IDX, LABEL_IDX = 0, 1\n",
    "        fields = [A_IDX, LABEL_IDX]\n",
    "        self.n_classes=n_classes\n",
    "        super(Dataset, self).__init__(path, field_indices=fields)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_labels():\n",
    "        \"\"\"Get classification label ids of the dataset.\"\"\"\n",
    "        return [str(_) for _ in range(self.n_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = Dataset(root=DATADIR, segment='train', n_classes=n_classes)\n",
    "data_dev = Dataset(root=DATADIR, segment='dev', n_classes=n_classes)\n",
    "data_test = Dataset(root=DATADIR, segment='test', n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<TEXT>>>>\n",
      "this superb film draws on a variety of talented actors and musicians at the top of their form levant , crosby , martin , rathbone , manone are completely at home in the story that apparently was supplied by billy wilder one would love to know more about how much he had to do with it , because it 's an exceptionally clever variation on the sterile master fertile servant tale nearly an allegory of the entertainment industry , run by dried up numskulls , but made into a vibrant world of art and play by an exploited underclass of nobodies and non wasps looking at the last six decades of music , tv , and film in the us , it 's hard not to see the underlying insights of this film as prophetic\n",
      "<<<<LABEL>>>>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "sample_id = np.random.randint(0, len(data_train))\n",
    "print('<<<<TEXT>>>>')\n",
    "print(data_train[sample_id][0])\n",
    "print(\"<<<<LABEL>>>>\")\n",
    "print(data_train[sample_id][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_base, vocabulary = nlp.model.get_model('bert_12_768_12',\n",
    "                                             dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                             pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                                             use_decoder=False, use_classifier=False)\n",
    "#print(bert_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the vocabulary from pre-trained model for tokenization\n",
    "tokenizer = tokenization.FullTokenizer(vocabulary, do_lower_case=True)\n",
    "transform = dataset.ClassificationTransform(tokenizer, all_labels, max_len, pair=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ids = \n",
      "[    2  2023 21688  2143  9891  2006  1037  3528  1997 10904  5889  1998\n",
      "  5389  2012  1996  2327  1997  2037  2433 24485  1010 14282  1010  3235\n",
      "  1010  9350  2232 14417  1010  2158  5643  2024  3294  2012  2188  1999\n",
      "  1996  2466  2008  4593  2001  8127  2011  5006 18463  2028  2052  2293\n",
      "  2000  2113  2062  2055  2129  2172  2002  2018  2000  2079  2007  2009\n",
      "  1010  2138  2009  1005  1055  2019 17077 12266  8386  2006  1996 25403\n",
      "  3040 14946  7947  6925  3053  2019  2035 20265  2854  1997  1996  4024\n",
      "  3068  1010  2448  2011  9550  2039 16371  5244  5283 12718  1010  2021\n",
      "  2081  2046  1037 17026  2088  1997  2396  1998  2377  2011  2019 18516\n",
      "  2104 26266  1997  2053  5092 18389  1998  2512 23146  2559  2012  1996\n",
      "  2197  2416  5109  1997  2189  1010  2694  1010  1998  2143  1999  1996\n",
      "  2149  1010  2009  1005  1055  2524  2025  2000  2156  1996 10318 20062\n",
      "  1997  2023  2143  2004 12168     3]\n"
     ]
    }
   ],
   "source": [
    "data_train = data_train.transform(transform)\n",
    "data_dev = data_dev.transform(transform)\n",
    "data_test = data_test.transform(transform)\n",
    "print('token ids = \\n%s'%data_train[sample_id][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = mx.gluon.data.DataLoader(data_train, batch_size=batch_size, shuffle=True, last_batch='rollover')\n",
    "dev_dataloader = mx.gluon.data.DataLoader(data_dev, batch_size=batch_size, shuffle=False, last_batch='rollover')\n",
    "test_dataloader = mx.gluon.data.DataLoader(data_dev, batch_size=batch_size, shuffle=False, last_batch='rollover')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bert.BERTClassifier(bert_base, num_classes=n_classes, dropout=0.1)\n",
    "# only need to initialize the classifier layer.\n",
    "model.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "model.hybridize(static_alloc=True)\n",
    "\n",
    "# softmax cross entropy loss for classification\n",
    "loss_function = gluon.loss.SoftmaxCELoss()\n",
    "loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "metric = mx.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 50/664] loss=0.6720, lr=0.0000050, acc=0.591\n",
      "[Epoch 0 Batch 100/664] loss=0.5595, lr=0.0000050, acc=0.672\n",
      "[Epoch 0 Batch 150/664] loss=0.4451, lr=0.0000050, acc=0.719\n",
      "[Epoch 0 Batch 200/664] loss=0.3723, lr=0.0000050, acc=0.753\n",
      "[Epoch 0 Batch 250/664] loss=0.3915, lr=0.0000050, acc=0.767\n",
      "[Epoch 0 Batch 300/664] loss=0.3544, lr=0.0000050, acc=0.781\n",
      "[Epoch 0 Batch 350/664] loss=0.3347, lr=0.0000050, acc=0.792\n",
      "[Epoch 0 Batch 400/664] loss=0.3473, lr=0.0000050, acc=0.799\n",
      "[Epoch 0 Batch 450/664] loss=0.3227, lr=0.0000050, acc=0.806\n",
      "[Epoch 0 Batch 500/664] loss=0.3221, lr=0.0000050, acc=0.813\n",
      "[Epoch 0 Batch 550/664] loss=0.3175, lr=0.0000050, acc=0.818\n",
      "[Epoch 0 Batch 600/664] loss=0.3079, lr=0.0000050, acc=0.822\n",
      "[Epoch 0 Batch 650/664] loss=0.3357, lr=0.0000050, acc=0.825\n",
      "EVALUATION ON DEV DATASET:\n",
      "dev mean loss: 0.3111, f1-score: 0.8740, accuracy: 0.8747\n",
      "[Epoch 1 Batch 50/664] loss=0.2537, lr=0.0000050, acc=0.896\n",
      "[Epoch 1 Batch 100/664] loss=0.2973, lr=0.0000050, acc=0.885\n",
      "[Epoch 1 Batch 150/664] loss=0.2859, lr=0.0000050, acc=0.883\n",
      "[Epoch 1 Batch 200/664] loss=0.2794, lr=0.0000050, acc=0.884\n",
      "[Epoch 1 Batch 250/664] loss=0.2524, lr=0.0000050, acc=0.887\n",
      "[Epoch 1 Batch 300/664] loss=0.2546, lr=0.0000050, acc=0.888\n",
      "[Epoch 1 Batch 350/664] loss=0.2857, lr=0.0000050, acc=0.887\n",
      "[Epoch 1 Batch 400/664] loss=0.2756, lr=0.0000050, acc=0.887\n",
      "[Epoch 1 Batch 450/664] loss=0.2744, lr=0.0000050, acc=0.886\n",
      "[Epoch 1 Batch 500/664] loss=0.2897, lr=0.0000050, acc=0.886\n",
      "[Epoch 1 Batch 550/664] loss=0.2783, lr=0.0000050, acc=0.885\n",
      "[Epoch 1 Batch 600/664] loss=0.2713, lr=0.0000050, acc=0.885\n",
      "[Epoch 1 Batch 650/664] loss=0.2626, lr=0.0000050, acc=0.886\n",
      "EVALUATION ON DEV DATASET:\n",
      "dev mean loss: 0.2832, f1-score: 0.8865, accuracy: 0.8865\n",
      "[Epoch 2 Batch 50/664] loss=0.2308, lr=0.0000050, acc=0.909\n",
      "[Epoch 2 Batch 100/664] loss=0.2389, lr=0.0000050, acc=0.907\n"
     ]
    }
   ],
   "source": [
    "trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': lr, 'epsilon': 1e-9})\n",
    "\n",
    "# collect all differentiable parameters\n",
    "# grad_req == 'null' indicates no gradients are calculated (e.g. constant parameters)\n",
    "# the gradients for these params are clipped later\n",
    "params = [p for p in model.collect_params().values() if p.grad_req != 'null']\n",
    "\n",
    "train_step = 0\n",
    "epoch_id = 0\n",
    "best_loss = None\n",
    "patience = 0\n",
    "while True:\n",
    "    metric.reset()\n",
    "    step_loss = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(train_dataloader):\n",
    "        # load data to GPU\n",
    "        token_ids = token_ids.as_in_context(ctx)\n",
    "        valid_length = valid_length.as_in_context(ctx)\n",
    "        segment_ids = segment_ids.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "\n",
    "        with autograd.record():\n",
    "            # forward computation\n",
    "            out = model(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "            ls = loss_function(out, label).mean()\n",
    "\n",
    "        # backward computation\n",
    "        ls.backward()\n",
    "\n",
    "        # gradient clipping\n",
    "        grads = [p.grad(c) for p in params for c in [ctx]]\n",
    "        gluon.utils.clip_global_norm(grads, grad_clip)\n",
    "\n",
    "        # parameter update\n",
    "        trainer.step(1)\n",
    "        step_loss += ls.asscalar()\n",
    "        metric.update([label], [out])\n",
    "        if (batch_id + 1) % (log_interval) == 0:\n",
    "            print('[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f}'\n",
    "                         .format(epoch_id, batch_id + 1, len(train_dataloader),\n",
    "                                 step_loss / log_interval,\n",
    "                                 trainer.learning_rate, metric.get()[1]))\n",
    "            step_loss = 0\n",
    "        train_step +=1\n",
    "    epoch_id+=1\n",
    "    ########################\n",
    "    #### RUN EVALUATION ####\n",
    "    ########################\n",
    "    dev_loss = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(dev_dataloader):\n",
    "        # load data to GPU\n",
    "        token_ids = token_ids.as_in_context(ctx)\n",
    "        valid_length = valid_length.as_in_context(ctx)\n",
    "        segment_ids = segment_ids.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        # get logits and loss value\n",
    "        out = model(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "        ls = loss_function(out, label).mean()\n",
    "        dev_loss.append(ls.asscalar())\n",
    "        probs = out.softmax()\n",
    "        pred = nd.argmax(probs, axis=1).asnumpy()\n",
    "        y_true.extend(list(np.reshape(label.asnumpy(), (-1))))\n",
    "        y_pred.extend(pred)\n",
    "    dev_loss = np.mean(dev_loss)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print('EVALUATION ON DEV DATASET:')\n",
    "    print('dev mean loss: {:.4f}, f1-score: {:.4f}, accuracy: {:0.4f}'.format(dev_loss, f1, acc))\n",
    "    if best_loss is None or dev_loss < best_loss:\n",
    "        model.save_parameters('{}_best'.format(filename, train_step))\n",
    "        best_loss = dev_loss\n",
    "        print('dev best loss updated: {:.4f}'.format(best_loss))\n",
    "        patience=0\n",
    "    else:\n",
    "        if patience == max_patience:\n",
    "            model.save_parameters('{}_{}'.format(filename, train_step))\n",
    "            break\n",
    "        new_lr = trainer.learning_rate/2\n",
    "        trainer.set_learning_rate(new_lr)\n",
    "        print('patience #{}: reducing the lr to {}'.format(patience, new_lr))\n",
    "        patience+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best pre-trained model for evaluation\n",
    "best_ckpt = glob('{}*best'.format(filename))[0]\n",
    "model = bert.BERTClassifier(bert_base, num_classes=n_classes, dropout=0.1)\n",
    "model.load_parameters(best_ckpt, ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "    token_ids = token_ids.as_in_context(ctx)\n",
    "    valid_length = valid_length.as_in_context(ctx)\n",
    "    segment_ids = segment_ids.as_in_context(ctx)\n",
    "    label = label.as_in_context(ctx)\n",
    "    out = model(token_ids, segment_ids, valid_length.astype('float32')).softmax()\n",
    "    pred = nd.argmax(out, axis=1).asnumpy()\n",
    "    y_true.extend(list(np.reshape(label.asnumpy(), (-1))))\n",
    "    y_pred.extend(pred)\n",
    "assert len(y_true)==len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = print_results(np.reshape(y_true, (-1)), y_pred)\n",
    "fig.savefig('{}/cm.png'.format(working_dir))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
